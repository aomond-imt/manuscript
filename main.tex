\documentclass{article}
\usepackage[letterpaper, left=1in, right=1in, bottom=1.1in, top=0.75in]{geometry}
\usepackage{graphicx} % Required for inserting images

\title{Manuscript Antoine Omond}
\author{Antoine OMOND}
\date{April 2024}

\input{header}

\begin{document}

\maketitle

\section{State of the art}
\subsection{Reconfiguration solutions for self-adaptive systems}

\subsubsection{MAPE-K patterns to represent and categorise solutions}

When dealing with self-adaptive systems, it is common to refer to the MAPE-K model~\cite{kephart_vision_2003}. The MAPE-K model is an abstraction proposed by IBM to help designing self-adaptive systems. The MAPE-K model is composed of four functions (MAPE) that have a common knowledge (K). The four functions are the Monitor (M), Analyse (A), Plan (P) and Execution (E) functions. Each function represents a specific step of the adaptation. The common knowledge represents the set of data or framework that are shared between the MAPE functions. This knowledge enables the collaboration between the different MAPE functions.
 
The MAPE model is often represented as a loop. Each function of the MAPE relies on the output of the previous one. Each function is responsible for its own part of the adaptation and provides results to the next function. 

The Monitor function is responsible for collecting information from the system. Such information can be extracted from the underlying host (\eg resource usage, remaining energy, software version, failure), the network (\eg neighbors discovery, contention, connectivity) or the physical environment (\eg heat). These information can be combined and transformed to construct the different metrics relevant for the adaptation. Such metrics can be related to the Quality of Service, software obsolescence, node's lifetime maximisation, etc. These metrics are provided to the next function of the loop, the Analysis function.

The Analysis function is responsible for the decision about whether or not to adapt based on the received metrics from the Monitor function. The Analysis function usually has a set of rules or goals that the system should to respect. The received metrics help the Analysis function to evaluate whether or not these goals are respected. Based on this evaluation, the Analysis function decides whether or not an adaptation is required to reach the desired goals. 

When an adaptation is necessary, the Analysis function computes the target state the system should reach. In the context of distributed services, the state of a system can be represented by the configuration of all services deployed on the system (\eg software version, active/inactive, etc). In this case, the target state of the system represents a configuration of services that should help matching the desired goals. The computation of the different steps and actions to go from the current to the target state is done by the next function, the Plan.

The Plan is responsible for the creation and scheduling of the different actions and steps to go from the current to the target state given by the Analysis function. The actions aim at reconfiguring part of the system (\eg installation, update, activation of services). The reconfigurable parts of the system and the rules to modify these should be exposed. Notably, the system architecture and relationship between its parts should be exposed. Adapting dependent services can lead to failure when proceed uncorrectly. Applying proper scheduling of adaptation actions ensures the safety and consistency of the system at each step of the Plan. The scheduling of actions is given to the final function of the loop, the Execution function.

The Execution function is responsible for the execution of actions given by the Plan. Each action implements concrete modifications on the system. The Execution function is responsible to ensure that each modification is done following the proper schedule from the Plan (\eg synchronisation between actions). Once the Execution is completed, the loop goes back to the Monitor function.

To enable collaboration between MAPE functions, data are shared between functions. The Monitor function needs to know what kind of metrics the Analysis function requires. The Analysis function requires knowledge about the current state of the system, including the deployed architecture and its configuration. The Plan and Execution functions need a common framework to define actions and execute them.

As said previously, the MAPE loop is an abstraction. It doesn't give any information about how each function is implemented on the underlying system. In the literature, adaptation solution often have different patterns, that suit different use-cases.
 
\subsubsection{Centralized MAPE-K}

A common pattern for the design and implementation of an autonomous system is to have a central controller managing a single or distributed infrastruture~\cite{garlan2004rainbow}. All MAPE functions are implemented in this controller. The controller has all the information and is solely responsible for the adaptation of the target infrastructure. Such pattern has the advantage of being simple to implement and decreases the complexity of the system. Lots of solutions from the literature and industry have implemented such patterns in their use-cases.

Some solutions from the literature implement only part of the MAPE loop. DevOps tools such as Terraform and Ansible allow to define the Plan and Execution parts of the MAPE loop. Both tools allow to define the different elements that are reconfigurable on the target infrastructure. 

R-MOZART~\cite{rmozart2021r} is a solution for reconfiguring IoT systems over the web. A GUI is provided to visualise the system and its configuration. Users can modify the system by adding or removing IoT devices, their properties and relationships in an interactive manner. The solution provides automatic verification of the new configuration. It then automatically computes and executes the Plan to reach the target state.

\subsubsection{Hierarchical MAPE-K}

In some use-cases, autonomous systems with a single central controller (\eg single MAPE-K loop) may be not be suited. Motivations for decentralised autonomous sytems notably imply scalability and reliability (\eg single point of failure) issues~\cite{quin2021mappingstudy}. 
%These systems may involve few to hundreds of devices. Providing autonomy to such systems poses a scalability issue. 
Different authorities and regulation may span across the system. These authorities may enforce local rules and/or objectives on the system. Data across the system may be distributed over different regions. Some regions may want to keep their data private, preventing their sharing. Systems may be potentially geo-distributed across different environment (\eg Cloud, edge, IoT). Connectivity between different parts of the system may be scarce or sporadic. 
%For these use-cases, having a single entity controlling the whole system (\eg single MAPE loop) may not be suitable.

In the literature, autonomous systems have been designed to answer the necessity for decentralised adaptations. In~\cite{weyns_patterns_2013}, the authors propose a methodology to represent decentralised MAPE patterns. Within a single architecture, multiple MAPE loops co-exist and manage a target distributed infrastructure. Functions are not bound to a single loop but can be located on different part of the system. Providing flexibility on the location of each MAPE function allows the design of multiple type of architectures, suitable for different use-cases. For instance, in the master/slave pattern, the Monitor and Execution functions are decentralized across distributed infrastructure, while the Analysis and Plan functions are located in a central controller. The controller gathers data from all target infrastructures via the Monitor functions, decides whether or not to adapt, computes the Plan and distributes this plan to the Execution functions. 

In ~\cite{tamiru_mck8s_2021}, authors propose a hierarchical MAPE-K to handle distributed Kubernetes clusters. A central controller manages several independant clusters. This controller is responislbe for the horizontal scaling intra and inter-clusters. It is also responsible for de-provisionning unused infrastructure. It monitors the data from the different clusters, and sends the appropriate adaptation actions to execute on each cluster.

In ~\cite{rossi2020hierarchical}, authors proposes a hierarchical MAPE-K to increase scalability of micro-service applications. Each micro-service is handled by a single MAPE-K loop. Each MAPE-K loop reports to a central MAPE global controller its local decisions regarding scaling of its containers for its microservice. These decisions are based on the input rate and response time of the micro-service. The central controller then forwards the commands to execute on the micro-service MAPE loop.

Some papers from the literature aim at managing containers at the Edge~\cite{rossi_self-management_2019}. In~\cite{xiong_kubeedge_2018}, the authors present Kubeedge. It aims at extending the capabilities of Kubernetes for Edge and IoT use-cases. A single controller is deployed in the Cloud, and sends adaptation actions to several Edge clusters. Connectivity between Cloud and Edge may periodically be lost. The authors leverage the capabilities of etcd to synchronise data between Cloud and Edge when connectivity is there, and locally stores data when connectivity is lost. This allows tolerance to network partition, and enable a degree of self-management on Edge nodes. This is an example of a master/slave MAPE-K. Where a master controller decides the new state to target, creates the adaptation actions, and sends them to slave Edge nodes which executes these instructions.

In~\cite{jimenez_docma_2019}, the authors consider a solution to host applications in a decentralised manner. Each node hosts a controller able to receive requests from user. Once a node receive a request to host an application, a leader election starts. The node elected leader distributed the application to different nodes, and is responsible for the self-healing property of the deployment. Deployed components of the application are re-deployed in the case of failure, and the health of the system is continuously monitored by the node. Any node of the network can become leader and any node can host an application. 

\subsubsection{Centralised Plan and/or Execution}

In CPSes, devices are provided with different capabilities. These capabilities include being able to change the configuration of software on the device, but also actuate their sensors, dowload additional software, or execute ad-hoc procedures. In order to be able to provide devices with such capabilities, a generic adaptation solution should be made. This solution should be able to coordinate and schedule any type of task. Expressiveness regarding the scheduling of tasks and their dependencies enables parallelism and time-efficient executions to quickly reach a desired configuration. Two levels of parallelism are considered: task-level and component-level. A component contains a set of tasks to execute. Parallelism at task-level allows to execute tasks in parallel, in the same node or between distant nodes. Parallelism at component level allows to execute components in parallels, in the same node or between distant nodes. Finally, the ability to provide any kind of scheduling may lead to inconsistencies during the execution. Providing a verification solution to verify the coordination of tasks before running could alleviate this problem. In the literature, several solutions have been designed to allow the modelling and execution of such tasks on single or distributed devices. These solutions come from the literature and the DevOps community.

Authors in ~\cite{stg_coordinating_2004} represents the activities of a distributed system using a directed graph. Nodes of the graph represents computation, communication or synchronisation activities. Edges of the graph represents control flow or synchronisation. Authors developed a language to modify the graph. Each function of the software presents on the device can be created or updated by modifying the directed graph. The solution compiles the directive of adaptation language into the adaptation code (\ie C code) to implement the directive. This allows fine-grained modifications of the underlying software. The solution can currently replace individual function of the underlying software. It is thus suitable for any kind of adaptation task. As the directed graph is an abstraction, it could theoretically be applied to any underlying language (providing support for this language).

The coordination is done using a central controller. This controller is responsible to compute the logical time at which the different nodes should execute adaptation tasks. The controller sends this information to the nodes of the system, which execute these tasks at the corresponding time. This allows the authors to perform an adaptation without requiring explicit coordination between nodes, but only by doing pre-computation by a central node. This adaptation process is co-located with node activities. Doing adaptation alongside node's activity can slow it down.Authors conducted experiments to compute the time overhead of their adaptation process. Experiments were conducted on low-CPU devices (\ie sub-GHz CPU). 

Ansible is a DevOps tool allowing the definition and execution of commands on a target devices. It is serverless, as it only relies only on an ssh connection with the target device to perform operations. It allows generic definition of operations on target devices, but does not allow parallelism inside a single component. It is limited to deployment only, and cannot express dependencies between different components. Verification before execution can be achieve to a certain extent, using the dry-run feature of the tool. Such feature provides a preview of some upcoming changes before applying them. 

Chef is a DevOps tool with a client-server architecture. As for Ansible, it allows generic adaptation actions to be performed on the device. It doesn't allow parallelism between tasks on a single component. It allows the expression of dependencies between components.

Several adaptation solutions are defined as a graph of components. Edges of the graph represent the relationship between components (e.g. dependencies). Each component has a predefined set of adaptation tasks and their scheduling (e.g. start/stop). Users can define actions to execute for each task. Relationships are at component-level, and the execution of each component is done independently in parallel in the case where there is no dependency between them. 

The standard TOSCA, its implementations, and solutions such as Brooklyn provides such model for Cloud applications. Each component can represent any type of element (VM, container, software, etc) and any kind of action can be executed on the tasks. The default scheduling provided by the standard can be extended (i.e. custom adaptation tasks can be added to the initial ones). Component adaptations can be executed in parallel.

In several solutions from the literature involving graphs of components, the tasks inside components are fixed and cannot be altered. Solutions such as JuJu, Terraform, PaaSage, AWS CloudFormation, OpenStack Heat, Kubernetes, Docker Swarm, MoDEMO, Puppet, SaltStack and JolieRedeployment Optimiser all have fixed components tasks. However, they can all execute any kind of adaptation.

Some adaptation solutions provide DSL to simplify the definition of their infrastructure. Solutions such as Terraform uses its own language to model the infrastructure and execute adaptation tasks. Pulumi provides general-purpose languages (Python, Typescript) to describe the infrastructure. PaaSage uses abstraction such as VM that is implemented for different cloud providers.

Some solutions from the literature allows to define dependencies at the task level, but only between different components. SmartFrog, Engage and Aeolus control the adaptation of their components using a state-machine. On each of these state machines, dependencies can be attached to transitions of a component with transitions of an other component.

Concerto~\cite{chardet_toward_2021} is an academic reconfiguration solution, that enhances parallelism by having inter-task dependencies in the same or between components. This solution allows to define any type of tasks. It initially targets Cloud applications, but can be used for any kind of infrastructure. This solution allows fine-grained control over the definition any kind of scheduling for any kind of tasks, and over the dependencies resolution. Different actions can be defined, such as the addition and removal of components, the connection between components, and the adaptation of one or multiple components. The semantic of Concerto allows to select and execute only a subset of adaptation tasks for each component. Once these tasks are selected, the engine automatically coordinates the execution of each task and resolves dependencies. For these reasons, this solution allows generic adaptation (e.g. any kind of tasks to be executed for any infrastructure), where tasks can be efficiently executed (\ie using fine-grained parallelism).

An other interesting property of Concerto is its performance graph computation. When an adaptation is executed in Concerto, a Directed Acyclic Graph is first created. Each edge represents either a task to execute and a dependency between components. Each edge is oriented toward the next task to execute. Edge representing tasks to execute are associated with a weight, representing the time required to execute the task. Computing the longest path in the graph gives the theoretical total execution time.

On top of the performance model, the graph computation provides three additional benefits. First, the graph computation allows to detect any cycle in the graph and alert the user if there is one. Second, having such graph allows to take decisions based on the theoretical duration. In cases where the theoretical duration may be too much, the adaptation can be modified to suit any time-constraint context. Third, the graph can be extended to provide additional features. Energy consumption of adaptation tasks can be put on the edges alongside their duration. Properties such as latency of dependencies could also be easily added.

\subsection{Decentralized execution (E) of a reconfiguration}

In~\cite{homeostatis2019tuning}, authors provide IoT nodes capabilities for dynamic collaboration. Services running on devices are dynamically adapted according to specified rules. On each device, several MAPE-K loops are running, one for each collaboration feature. Features such as collaborative sensing by dynamically relying on neighbors' sensor data can be enabled. Connection between services can be dynamically created, in a fully decentralised manner. Devices are considered mobile, and collaborate through ad-hoc connections. While the Monitor and Analysis capabilities and the dynamicity of the system are clearly defined, the Plan and Execution steps, especially communication between devices is not handled. 

In~\cite{colombo2022p2pmonitoring}, authors grant self-adaptation capabilities to each Edge node by implementing MAPE-K loop for each. Nodes have a degree of collaboration for the Monitor part. Nodes aim at lowering their energy consumption and bandwidth utilisation by implementation adaptation policies that allow them to reach a low-energy consuming state. The Analysis, Plan and Execute part is done individually on nodes, and consists in adapting the frequency of data report by the nodes and the actuation of the probing system.

To provide a wide range of adaptation capabilities to devices, some solutions provide a fully decentralised adaptation. This means that each node is capable of handling its own adaptation, and coordinate  with other nodes. In~\cite{sokolowski_mjuz_2021}, the authors present Muse, a decentralised adaptation solution. Each node is able to declare its services and parameters using a declarative syntax. Local dependencies can be expressed between services on the same node to coordinate the adaptation locally. Remote dependencies can be expressed to enable collaboration between different nodes. The execution of adaptation is done using a deployment graph. Mjuz frequently compares the current state of the infrastructure with the deployment graph, deploy/update/delete the components on the infrastructure and update the current state. When dependencies are not resolved, the nodes relying on these dependencies are cut off the graph. This ensures that components with unresolved dependencies are not handled until their dependencies are resolved.

To resolve remote dependencies, each node is equipped with a reactive engine. This engine periodically listens through a gRPC connection for remote dependencies from its dependent nodes. The engine dynamically updates the deployment graph according to the resolve dependencies. The execution of the engine is decorellated from the deployed infrastructure. This means that the engine can be gracefully stopped and restarted when needed. Whenever an adaptation task is completed (\eg deployment/update/delete of component), the state of the infrastructure is saved (either locally or remotely). The solution is optimised for Cloud systems and resource provisioning (\eg VM, container) but can be extended to handle any resource by implementing a CRUD API. Each deployed component is immutable, and its adaptation can be controlled using CRUD actions. Components with resolved dependencies are handled in parallel. The dependencies are resolved at the component level. The solution currently uses Pulumi as back-end for the execution of adaptation tasks. Pulumi is an infrastructure-as-code solution that targets Cloud systems.

Another example of declarative distributed adaptation is presented in ~\cite{crossorga_decentralized_2020}. In this paper, the authors uses decentralisation to tackle the cooperation of multiple authorities on a single application. While this paper targets application deployed in the Cloud, the model is sufficiently generic to be applied to any kind of infrastructure. Each organisation aims at keeping the data about their infrastructure private. To do so, a model is provided for the organisations to design the components of the application and their relationship. These relationship can be either between different services (\eg API) or between layers (\eg database relying on a VM). The life-cycle and relationship of each component are defined. However, the relationship are resolved only at the component-level. The relying infrastructure (\eg Cloud service) is abstracted at first and infrastructure is replaced by placeholders. Once the model is designed, it is distributed to each organisation participating in the application. Then, each application replace the placeholders by their own infrastructure (\eg AWS Cloud, OVH Cloud, Java service, etc). 

From the resulting model, a deployment workflow is created to execute the adaptation tasks. First, a synchronisation barrier between all participants is done before starting the adaptation. Each participation sends n-1 messages to notify that they are ready for the adaptation. Then, the adaptation starts. A sequence of tasks is generated for each model. For each dependency that a task relies on, an additional task is created to send or receive data to resolve the dependency. Tasks with resolved dependencies are executed in parallel. While the deployment of components is supported, the update of component or any other management task is not covered. The type of communication and communication pattern between nodes is abstracted.

In ~\cite{coordination_dsu_2016}, authors introduce a middleware to coordinate the update of IoT devices. This middleware rely on a Java framework (LyRT) to conduct the actual update on devices. It is embedded with a reactive engine that continuously watch changes on the service configurations. Whenever changes are detected, adaptation events are fired (\eg replace running instances, bind instance with new dependent service).

To coordinate changes between devices, the solution is application-agnostic and relies on global synchronisations pattern. The dependencies between services across multiple nodes are still expressed, but their resolution is done using global synchronisation. This allows simpler and more predicable dependencies resolution, as the same pattern is executed for any kind of adaptation. An update using this resolution is done in several steps. First, a Plan is assumed to be computed by a central entity (external to the system) and sent to a node of the system. This plan contains the meta-data of the update and the involved nodes in the update. The node receiving the Plan distribute it to other nodes involved in the update. The nodes uses the meta-data to download the update from a central repository. A first synchronisation barrier occurs to make sure all nodes finished downloading the update. Then, the nodes perform the update without commiting. This is done using the underlying LyRT framework, allowing to inject changes, but first without activating them. A second synchronisation barrier occurs to make sure nodes all applied changes without activating them. Finally, a third synchronisation barrier is done when changes are activated by the framework.

While authors target IoT systems, they target large devices such as IoT Connectors or Cloud Platform. Typical devices such as sensor nodes or certain mobile devices are not considered to be suitable for their solutions, due to the required computing power to compute the update. The adaptable elements are Java class files that can be created, updated, deleted, and their dependencies re-bound. Thus, any adaptation capabilities can be provided as long as a binding exist with the Java language. The communications between devices and their pattern is abstracted and not discussed.



%Authors in ~\cite{xiong_kubeedge_2018} presents Kubeedge, a Kubernetes-based self-adaptation solution for controlling from the Cloud Kubernetes clusters deployed at the edge. The controller at the Cloud and the Edge devices are part of a single network, created by a dedicated protocol. Each Edge device has a specific deamon. The goal of this deamon is to receive requests and data from the central Cloud controller, and execute these. Knowledge about the state of the infrastructure is shared between the Cloud and Edge devices by leveraging etcd capabilities for replication. This allows the solution to be tolerant to network partitions (\ie losing connectivity with the controller) by allowing the Edge devices to function on their own during the partition.

%Some steps can be done in parallel, others are done sequentially. Some steps of the Plan can be related to each other. In this case, the Execution has to wait for the tasks it relies on before continuing. 

\bibliographystyle{IEEEtran}
\bibliography{main.bib}

\end{document}
